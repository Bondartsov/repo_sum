"""
–¢–µ—Å—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å OpenAI - –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫ –∏ rate limit.

T-017 - OpenAI: rate limit –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
T-018 - OpenAI: –æ—Ñ–ª–∞–π–Ω/–Ω–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
"""

import pytest
import asyncio
import time
from unittest.mock import patch, MagicMock, AsyncMock, call
import httpx

from openai import RateLimitError, APIConnectionError, APITimeoutError
from openai_integration import OpenAIManager
from utils import CodeChunk, GPTAnalysisRequest, GPTAnalysisResult


@pytest.fixture
def sample_request():
    """–ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    chunk = CodeChunk(
        name="test_function",
        content="def test_func():\n    return True",
        start_line=1,
        end_line=2,
        chunk_type="function"
    )
    return GPTAnalysisRequest(
        file_path="test.py",
        language="python",
        chunks=[chunk],
        context="Test context"
    )


@pytest.fixture
def mock_config():
    """–ú–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ retry"""
    config = MagicMock()
    config.openai.api_key = "test_key"
    config.openai.model = "gpt-3.5-turbo"
    config.openai.temperature = 0.7
    config.openai.max_response_tokens = 1000
    config.openai.retry_attempts = 3
    config.openai.retry_delay = 1.0
    config.analysis.sanitize_enabled = False
    config.prompts.code_analysis_prompt_file = "prompts/code_analysis_prompt.md"
    return config


@pytest.mark.integration
class TestOpenAIRateLimit:
    """T-017 - OpenAI: rate limit –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã"""

    @patch('openai_integration.get_config')
    @patch('openai_integration.load_prompt_from_file')
    @patch('openai_integration.OpenAI')
    def test_rate_limit_with_retries_success(self, mock_openai_class, mock_load_prompt, mock_get_config, sample_request, mock_config):
        """
        –¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ rate limit –æ—à–∏–±–æ–∫.
        
        –°—Ü–µ–Ω–∞—Ä–∏–π:
        - –ü–µ—Ä–≤—ã–µ 2 –ø–æ–ø—ã—Ç–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç RateLimitError
        - 3-—è –ø–æ–ø—ã—Ç–∫–∞ —É—Å–ø–µ—à–Ω–∞
        - –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ –∑–∞–¥–µ—Ä–∂–∫–∏
        """
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∫–æ–≤
        mock_get_config.return_value = mock_config
        mock_load_prompt.return_value = "Test prompt template: {filename} {code_content}"
        
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client
        
        # –ü–µ—Ä–≤—ã–µ 2 –≤—ã–∑–æ–≤–∞ - rate limit, 3-–π —É—Å–ø–µ—à–µ–Ω
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = "üîç –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ —É—Å–ø–µ—à–µ–Ω"
        
        # –°–æ–∑–¥–∞–µ–º mock response —Å status_code –∏ headers –¥–ª—è RateLimitError
        mock_response_obj = MagicMock(spec=httpx.Response)
        mock_response_obj.status_code = 429
        mock_response_obj.headers = {"x-request-id": "test-request-id"}
        
        side_effects = [
            RateLimitError("Rate limit exceeded", response=mock_response_obj, body=None),
            RateLimitError("Rate limit exceeded", response=mock_response_obj, body=None),
            mock_response
        ]
        mock_client.chat.completions.create.side_effect = side_effects
        
        # –ú–æ–∫–∏—Ä—É–µ–º asyncio.sleep –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–¥–µ—Ä–∂–µ–∫
        with patch('asyncio.sleep') as mock_sleep:
            manager = OpenAIManager()
            # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
            with patch.object(manager.cache, 'get_cached_result', return_value=None):
                result = asyncio.run(manager.analyze_code(sample_request))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        assert isinstance(result, GPTAnalysisResult)
        assert result.error is None
        assert "–ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ —É—Å–ø–µ—à–µ–Ω" in result.summary
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–∑–æ–≤–æ–≤ API (3 –ø–æ–ø—ã—Ç–∫–∏)
        assert mock_client.chat.completions.create.call_count == 3
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–¥–µ—Ä–∂–∫–∏ (2 –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É 3 –ø–æ–ø—ã—Ç–∫–∞–º–∏)
        assert mock_sleep.call_count == 2
        expected_calls = [call(1.0), call(1.0)]  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        mock_sleep.assert_has_calls(expected_calls)

    @patch('openai_integration.get_config')
    @patch('openai_integration.load_prompt_from_file') 
    @patch('openai_integration.OpenAI')
    def test_rate_limit_exhausted_retries(self, mock_openai_class, mock_load_prompt, mock_get_config, sample_request, mock_config):
        """
        –¢–µ—Å—Ç –∏—Å—á–µ—Ä–ø–∞–Ω–∏—è –ª–∏–º–∏—Ç–∞ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ rate limit.
        
        –°—Ü–µ–Ω–∞—Ä–∏–π:
        - –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç RateLimitError
        - –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è –∏—Å—Ö–æ–¥–Ω–∞—è –æ—à–∏–±–∫–∞
        - –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É
        """
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∫–æ–≤
        mock_get_config.return_value = mock_config
        mock_load_prompt.return_value = "Test prompt template: {filename} {code_content}"
        
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client
        
        # –í—Å–µ –≤—ã–∑–æ–≤—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç rate limit error
        mock_response_obj = MagicMock(spec=httpx.Response)
        mock_response_obj.status_code = 429
        mock_response_obj.headers = {"x-request-id": "test-request-id"}
        rate_limit_error = RateLimitError("Rate limit exceeded", response=mock_response_obj, body=None)
        mock_client.chat.completions.create.side_effect = rate_limit_error
        
        with patch('asyncio.sleep') as mock_sleep:
            manager = OpenAIManager()
            # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
            with patch.object(manager.cache, 'get_cached_result', return_value=None):
                result = asyncio.run(manager.analyze_code(sample_request))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π (–Ω–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ)
        assert isinstance(result, GPTAnalysisResult)
        assert result.error is not None
        assert "Rate limit exceeded" in result.error
        assert result.summary == "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ (—Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É retry_attempts = 3)
        assert mock_client.chat.completions.create.call_count == 3
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–¥–µ—Ä–∂–∫–∏ (2 –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É 3 –ø–æ–ø—ã—Ç–∫–∞–º–∏)
        assert mock_sleep.call_count == 2

    @patch('openai_integration.get_config')
    @patch('openai_integration.load_prompt_from_file')
    @patch('openai_integration.OpenAI')
    def test_rate_limit_retry_configuration(self, mock_openai_class, mock_load_prompt, mock_get_config, sample_request):
        """
        –¢–µ—Å—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ –∑–∞–¥–µ—Ä–∂–µ–∫ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
        """
        # –ö–æ–Ω—Ñ–∏–≥ —Å –¥—Ä—É–≥–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ retry
        config = MagicMock()
        config.openai.api_key = "test_key"
        config.openai.model = "gpt-3.5-turbo" 
        config.openai.temperature = 0.7
        config.openai.max_response_tokens = 1000
        config.openai.retry_attempts = 5  # –ë–æ–ª—å—à–µ –ø–æ–ø—ã—Ç–æ–∫
        config.openai.retry_delay = 2.0   # –ë–æ–ª—å—à–µ –∑–∞–¥–µ—Ä–∂–∫–∞
        config.analysis.sanitize_enabled = False
        config.prompts.code_analysis_prompt_file = "prompts/code_analysis_prompt.md"
        
        mock_get_config.return_value = config
        mock_load_prompt.return_value = "Test prompt: {filename} {code_content}"
        
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client
        
        # –í—Å–µ –≤—ã–∑–æ–≤—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç rate limit
        mock_response_obj = MagicMock(spec=httpx.Response)
        mock_response_obj.status_code = 429
        mock_response_obj.headers = {"x-request-id": "test-request-id"}
        rate_limit_error = RateLimitError("Rate limit exceeded", response=mock_response_obj, body=None)
        mock_client.chat.completions.create.side_effect = rate_limit_error
        
        with patch('asyncio.sleep') as mock_sleep:
            manager = OpenAIManager()
            # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
            with patch.object(manager.cache, 'get_cached_result', return_value=None):
                result = asyncio.run(manager.analyze_code(sample_request))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–æ–≤–æ–º—É –∫–æ–Ω—Ñ–∏–≥—É
        assert mock_client.chat.completions.create.call_count == 5
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–¥–µ—Ä–∂–∫–∏ —Å –Ω–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
        assert mock_sleep.call_count == 4  # 4 –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É 5 –ø–æ–ø—ã—Ç–∫–∞–º–∏
        expected_calls = [call(2.0)] * 4
        mock_sleep.assert_has_calls(expected_calls)


@pytest.mark.integration
class TestOpenAIConnectionErrors:
    """T-018 - OpenAI: –æ—Ñ–ª–∞–π–Ω/–Ω–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""

    @patch('openai_integration.get_config')
    @patch('openai_integration.load_prompt_from_file')
    @patch('openai_integration.OpenAI')
    def test_connection_error_handling(self, mock_openai_class, mock_load_prompt, mock_get_config, sample_request, mock_config):
        """
        –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è (–Ω–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞).
        
        –°—Ü–µ–Ω–∞—Ä–∏–π:
        - API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç APIConnectionError
        - –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏
        - –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–∫–∏
        """
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∫–æ–≤
        mock_get_config.return_value = mock_config
        mock_load_prompt.return_value = "Test prompt template: {filename} {code_content}"
        
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client
        
        # –í—Å–µ –≤—ã–∑–æ–≤—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç connection error
        mock_request = MagicMock(spec=httpx.Request)
        connection_error = APIConnectionError(message="Connection failed", request=mock_request)
        mock_client.chat.completions.create.side_effect = connection_error
        
        with patch('asyncio.sleep') as mock_sleep:
            manager = OpenAIManager()
            # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
            with patch.object(manager.cache, 'get_cached_result', return_value=None):
                result = asyncio.run(manager.analyze_code(sample_request))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π
        assert isinstance(result, GPTAnalysisResult)
        assert result.error is not None
        assert "Connection failed" in result.error
        assert result.summary == "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        assert mock_client.chat.completions.create.call_count == 3
        assert mock_sleep.call_count == 2

    @patch('openai_integration.get_config')
    @patch('openai_integration.load_prompt_from_file')
    @patch('openai_integration.OpenAI')
    def test_timeout_error_handling(self, mock_openai_class, mock_load_prompt, mock_get_config, sample_request, mock_config):
        """
        –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–∞–π–º–∞—É—Ç–æ–≤.
        
        –°—Ü–µ–Ω–∞—Ä–∏–π:
        - API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç APITimeoutError
        - –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
        """
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∫–æ–≤
        mock_get_config.return_value = mock_config
        mock_load_prompt.return_value = "Test prompt template: {filename} {code_content}"
        
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client
        
        # –í—Å–µ –≤—ã–∑–æ–≤—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç timeout error
        mock_request = MagicMock(spec=httpx.Request)
        timeout_error = APITimeoutError(request=mock_request)
        mock_client.chat.completions.create.side_effect = timeout_error
        
        with patch('asyncio.sleep') as mock_sleep:
            manager = OpenAIManager()
            # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
            with patch.object(manager.cache, 'get_cached_result', return_value=None):
                result = asyncio.run(manager.analyze_code(sample_request))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π
        assert isinstance(result, GPTAnalysisResult)
        assert result.error is not None
        assert "Request timed out" in result.error or "timeout" in result.error.lower()
        assert result.summary == "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        assert mock_client.chat.completions.create.call_count == 3
        assert mock_sleep.call_count == 2

    @patch('openai_integration.get_config')
    @patch('openai_integration.load_prompt_from_file')
    @patch('openai_integration.OpenAI')
    def test_mixed_network_errors_recovery(self, mock_openai_class, mock_load_prompt, mock_get_config, sample_request, mock_config):
        """
        –¢–µ—Å—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫.
        
        –°—Ü–µ–Ω–∞—Ä–∏–π:
        - 1-—è –ø–æ–ø—ã—Ç–∫–∞: ConnectionError
        - 2-—è –ø–æ–ø—ã—Ç–∫–∞: TimeoutError  
        - 3-—è –ø–æ–ø—ã—Ç–∫–∞: —É—Å–ø–µ—Ö
        """
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∫–æ–≤
        mock_get_config.return_value = mock_config
        mock_load_prompt.return_value = "Test prompt template: {filename} {code_content}"
        
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client
        
        # –°–º–µ—à–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏, –ø–æ—Ç–æ–º —É—Å–ø–µ—Ö
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = "üîç –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
        
        mock_request = MagicMock(spec=httpx.Request)
        side_effects = [
            APIConnectionError(message="No internet", request=mock_request),
            APITimeoutError(request=mock_request),
            mock_response
        ]
        mock_client.chat.completions.create.side_effect = side_effects
        
        with patch('asyncio.sleep') as mock_sleep:
            manager = OpenAIManager()
            # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
            with patch.object(manager.cache, 'get_cached_result', return_value=None):
                result = asyncio.run(manager.analyze_code(sample_request))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        assert isinstance(result, GPTAnalysisResult)
        assert result.error is None
        assert "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑" in result.summary
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–∑–æ–≤–æ–≤
        assert mock_client.chat.completions.create.call_count == 3
        assert mock_sleep.call_count == 2

    @patch('openai_integration.get_config')
    @patch('openai_integration.load_prompt_from_file')
    @patch('openai_integration.OpenAI')
    def test_no_infinite_waiting(self, mock_openai_class, mock_load_prompt, mock_get_config, sample_request):
        """
        –¢–µ—Å—Ç —á—Ç–æ –Ω–µ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –æ–∂–∏–¥–∞–Ω–∏–π –ø—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö.
        
        –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ.
        """
        # –ö–æ–Ω—Ñ–∏–≥ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
        config = MagicMock()
        config.openai.api_key = "test_key"
        config.openai.model = "gpt-3.5-turbo"
        config.openai.temperature = 0.7
        config.openai.max_response_tokens = 1000
        config.openai.retry_attempts = 2
        config.openai.retry_delay = 0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        config.analysis.sanitize_enabled = False
        config.prompts.code_analysis_prompt_file = "prompts/code_analysis_prompt.md"
        
        mock_get_config.return_value = config
        mock_load_prompt.return_value = "Test prompt: {filename} {code_content}"
        
        mock_client = MagicMock()
        mock_openai_class.return_value = mock_client
        
        # –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ connection errors
        mock_request = MagicMock(spec=httpx.Request)
        connection_error = APIConnectionError(message="Network unreachable", request=mock_request)
        mock_client.chat.completions.create.side_effect = connection_error
        
        # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        start_time = time.time()
        
        with patch('asyncio.sleep') as mock_sleep:
            manager = OpenAIManager()
            # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
            with patch.object(manager.cache, 'get_cached_result', return_value=None):
                result = asyncio.run(manager.analyze_code(sample_request))
        
        execution_time = time.time() - start_time
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å –±—ã—Å—Ç—Ä–æ (–±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–µ—Ä–∂–µ–∫ –±–ª–∞–≥–æ–¥–∞—Ä—è –º–æ–∫—É)
        assert execution_time < 1.0  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ —Å –º–æ–∫–∞–º–∏
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ–ø—ã—Ç–∫–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã
        assert mock_client.chat.completions.create.call_count == 2
        assert mock_sleep.call_count == 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—à–∏–±–∫—É –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
        assert result.error is not None
        assert "Network unreachable" in result.error
