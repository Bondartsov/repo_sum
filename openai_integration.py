"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å OpenAI ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞—É–¥–∏—Ç –∫–æ–¥–∞.
–ò–∑–º–µ–Ω–µ–Ω–∏—è: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –æ—Ç—á—ë—Ç–∞, —Ç–æ–∫–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç ‚Üë –¥–æ 2048.
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import openai
import tiktoken
from openai import OpenAI

from config import get_config
from utils import (
    CodeChunk,
    GPTAnalysisRequest,
    GPTAnalysisResult,
    OpenAIError,
    sanitize_text,
)

logger = logging.getLogger(__name__)

# –ü—Ä–æ–º–ø—Ç –≤—ã–Ω–µ—Å–µ–Ω –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª prompts/code_analysis_prompt.md


def load_prompt_from_file(prompt_file: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –∏–∑ —Ñ–∞–π–ª–∞"""
    try:
        with open(prompt_file, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except FileNotFoundError:
        logger.error(f"–§–∞–π–ª –ø—Ä–æ–º–ø—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {prompt_file}")
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ {prompt_file}: {e}")
        raise




class GPTCache:
    """–ö—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã GPT‚Äë–∞–Ω–∞–ª–∏–∑–æ–≤"""

    def __init__(self, cache_dir: str = "./cache") -> None:
        self.dir = Path(cache_dir)
        self.dir.mkdir(parents=True, exist_ok=True)

    def get_cache_key(self, request: GPTAnalysisRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        content = f"{request.file_path}_{request.language}_"
        content += "_".join([chunk.content for chunk in request.chunks])
        return hashlib.md5(content.encode()).hexdigest()

    def clear_expired_cache(self, days: int = 7) -> None:
        """–û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ –∫—ç—à–∞"""
        cutoff = datetime.now() - timedelta(days=days)
        for cache_file in self.dir.glob("*.json"):
            try:
                data = json.loads(cache_file.read_text(encoding="utf-8"))
                cached_at = datetime.fromisoformat(data.get("cached_at", ""))
                if cached_at < cutoff:
                    cache_file.unlink()
                    logger.debug("–£–¥–∞–ª–µ–Ω —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à: %s", cache_file.name)
            except Exception as exc:
                logger.warning("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫—ç—à–∞ %s: %s", cache_file.name, exc)

    def get_cached_result(self, key: str) -> Optional[GPTAnalysisResult]:
        file = self.dir / f"{key}.json"
        if not file.exists():
            return None
        try:
            data = json.loads(file.read_text(encoding="utf-8"))
            return GPTAnalysisResult(
                summary=data.get("summary", ""),
                key_components=data.get("key_components", []),
                analysis_per_chunk=data.get("analysis_per_chunk", {}),
                error=data.get("error"),
                full_text=data.get("full_text", ""),
            )
        except Exception as exc:
            logger.warning("–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞ %s: %s", key, exc)
            return None

    def cache_result(self, key: str, res: GPTAnalysisResult) -> None:
        file = self.dir / f"{key}.json"
        data = {
            "summary": res.summary,
            "key_components": res.key_components,
            "analysis_per_chunk": res.analysis_per_chunk,
            "error": res.error,
            "full_text": res.full_text,
            "cached_at": datetime.now().isoformat(),
        }
        try:
            file.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        except Exception as exc:  # pragma: no cover
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –∫—ç—à %s: %s", key, exc)


class OpenAIManager:
    """–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å OpenAI"""

    def analyze_chunk(self, request: GPTAnalysisRequest):
        """
        –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Ç–µ—Å—Ç–∞–º–∏: —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ analyze_code.
        """
        return asyncio.run(self.analyze_code(request))

    def __init__(self) -> None:
        self.config = get_config()
        if not self.config.openai.api_key:
            raise ValueError("OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")
        self.client = OpenAI(api_key=self.config.openai.api_key)
        self.model = self.config.openai.model
        self.temperature = self.config.openai.temperature
        try:
            self.encoder = tiktoken.encoding_for_model(self.model)
        except Exception:
            self.encoder = tiktoken.get_encoding("cl100k_base")

        self.cache = GPTCache()

    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        return len(self.encoder.encode(text))

    def truncate_to_tokens(self, text: str, max_tokens: int) -> str:
        """–û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        tokens = self.encoder.encode(text)
        if len(tokens) <= max_tokens:
            return text
        truncated_tokens = tokens[:max_tokens]
        return self.encoder.decode(truncated_tokens)

    async def analyze_code(self, request: GPTAnalysisRequest) -> GPTAnalysisResult:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞ —á–µ—Ä–µ–∑ GPT"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cache_key = self.cache.get_cache_key(request)
            cached = self.cache.get_cached_result(cache_key)
            if cached:
                logger.debug("–ù–∞–π–¥–µ–Ω –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è %s", request.file_path)
                return cached

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–¥
            combined_code = self._combine_chunks(request.chunks)
            
            # –°–∞–Ω–∏—Ç–∞–π–∑–∏–Ω–≥ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if self.config.analysis.sanitize_enabled:
                combined_code = sanitize_text(combined_code, self.config.analysis.sanitize_patterns)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = self._build_analysis_prompt(request, combined_code)
            
            # –í—ã–∑—ã–≤–∞–µ–º API
            response = await self._call_openai_api(prompt)
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            result = self._parse_gpt_response(response, request.chunks)
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.cache.cache_result(cache_key, result)
            
            return result

        except Exception as exc:
            error_msg = f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {request.file_path}: {str(exc)}"
            logger.error(error_msg)
            return GPTAnalysisResult(
                summary="–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
                key_components=[],
                analysis_per_chunk={},
                error=error_msg
            )

    def _combine_chunks(self, chunks: List[CodeChunk]) -> str:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∫–æ–¥–∞ –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç"""
        if not chunks:
            return ""
        
        # –ë–µ—Ä—ë–º –¥–æ 3-4 –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        important_chunks = chunks[:4]
        combined = []
        
        for chunk in important_chunks:
            combined.append(f"// --- {chunk.name} (—Å—Ç—Ä–æ–∫–∏ {chunk.start_line}-{chunk.end_line}) ---")
            combined.append(chunk.content)
            combined.append("")
        
        return "\n".join(combined)

    def _build_analysis_prompt(self, request: GPTAnalysisRequest, code: str) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        filename = Path(request.file_path).name
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        total_lines = code.count('\n') + 1 if code else 0
        functions_count = len([chunk for chunk in request.chunks if chunk.chunk_type == 'function'])
        classes_count = len([chunk for chunk in request.chunks if chunk.chunk_type == 'class'])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–¥–∞
        max_code_tokens = 1500  # –û—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞
        if self.count_tokens(code) > max_code_tokens:
            code = self.truncate_to_tokens(code, max_code_tokens)
            code += "\n\n... [–∫–æ–¥ –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤] ..."

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ —Ñ–∞–π–ª–∞
        prompt_template = load_prompt_from_file(self.config.prompts.code_analysis_prompt_file)
        
        return prompt_template.format(
            filename=filename,
            total_lines=total_lines,
            functions_count=functions_count,
            classes_count=classes_count,
            code_content=code
        )

    async def _call_openai_api(self, prompt: str) -> str:
        # –†–µ—Ç—Ä–∞–∏ –Ω–∞ —Å–ª—É—á–∞–π –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫ —Å–µ—Ç–∏/–∫–≤–æ—Ç
        attempts = max(1, int(self.config.openai.retry_attempts))
        delay = max(0.0, float(self.config.openai.retry_delay))
        last_exc: Optional[Exception] = None

        for attempt in range(1, attempts + 1):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∫–æ–¥–∞. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–π –∫—Ä–∞—Ç–∫–∏–µ –∏ —Ç–æ—á–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è.",
                        },
                        {"role": "user", "content": prompt},
                    ],
                    temperature=self.temperature,
                    max_tokens=self.config.openai.max_response_tokens,
                )
                return response.choices[0].message.content
            except Exception as exc:
                last_exc = exc
                logger.warning(
                    "–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ OpenAI (–ø–æ–ø—ã—Ç–∫–∞ %s/%s): %s", attempt, attempts, exc
                )
                if attempt < attempts:
                    await asyncio.sleep(delay)

        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã ‚Äî –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –≤—ã—à–µ
        assert last_exc is not None
        raise last_exc

    def _parse_gpt_response(self, text: str, chunks: List[CodeChunk]) -> GPTAnalysisResult:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ–º *–≤–µ—Å—å* text –≤ full_text, –∞ summary/keys ‚Äî –¥–ª—è –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏.
        """
        summary = ""
        key_components: List[str] = []

        for line in text.splitlines():
            if line.startswith("üîç") or line.startswith("–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:"):
                summary = line.lstrip("üîç ").replace("–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:", "").strip()
            if line.startswith("- ") and "–§—É–Ω–∫—Ü–∏—è" in line:
                key_components.append(line.lstrip("- ").strip())

        if not summary:
            summary = text[:200] + "..." if len(text) > 200 else text

        return GPTAnalysisResult(
            summary=summary,
            key_components=key_components,
            analysis_per_chunk={chunk.name: summary for chunk in chunks[:3]},
            full_text=text,
            error=None,
        )

    def get_token_usage_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (–∑–∞–≥–ª—É—à–∫–∞)"""
        return {
            "total_requests": 0,
            "total_tokens": 0,
            "average_tokens_per_request": 0
        }

    def clear_cache(self) -> int:
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ OpenAI"""
        cache_files = list(self.cache.dir.glob("*.json"))
        count = len(cache_files)
        for cache_file in cache_files:
            cache_file.unlink()
        return count
