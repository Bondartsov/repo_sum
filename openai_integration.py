"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å OpenAI ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞—É–¥–∏—Ç –∫–æ–¥–∞.
–ò–∑–º–µ–Ω–µ–Ω–∏—è: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –æ—Ç—á—ë—Ç–∞, —Ç–æ–∫–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç ‚Üë –¥–æ 2048.
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import openai
import tiktoken
from openai import OpenAI

from config import get_config
from utils import (
    CodeChunk,
    GPTAnalysisRequest,
    GPTAnalysisResult,
    OpenAIError,
)

logger = logging.getLogger(__name__)

# –ü—Ä–æ–º–ø—Ç –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞
MINIMAL_ANALYSIS_PROMPT = """
–¢—ã ‚Äî –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–≥–µ–Ω—Ç –ø–æ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ–º—É –∞—É–¥–∏—Ç—É –ª–æ–≥–∏–∫–∏ —Ä–∞–±–æ—Ç—ã –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞.  
–ù–µ —É–∫–∞–∑—ã–≤–∞–π –Ω–∞ –æ—à–∏–±–∫–∏, –Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞–π —É–ª—É—á—à–µ–Ω–∏—è –∏ –Ω–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã.  
–§–æ–∫—É—Å–∏—Ä—É–π—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —Ç–æ–º, —á—Ç–æ –¥–µ–ª–∞–µ—Ç —Ñ–∞–π–ª, –∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ–Ω –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏ –∫—É–¥–∞ –ø–µ—Ä–µ–¥–∞—ë—Ç, –∞ —Ç–∞–∫–∂–µ –Ω–∞ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–∏ –µ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤.

–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Ä–∞–∑–¥–µ–ª—ã:

üîç –û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ª–æ–≥–∏–∫–∏  
- –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ü–µ–ª–∏ –∏ —Ä–æ–ª–∏ —Ñ–∞–π–ª–∞ –≤ —Å–∏—Å—Ç–µ–º–µ.  
- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Ñ–∞–π–ª—ã, API, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Ç.–ø.) –∏ –∏—Ö —Ñ–æ—Ä–º–∞—Ç.  
- –ü—Ä–∏—ë–º–Ω–∏–∫–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–∫–æ–Ω—Å–æ–ª—å, —Ñ–∞–π–ª—ã, –¥—Ä—É–≥–∏–µ –º–æ–¥—É–ª–∏, –ë–î, API) –∏ —Ñ–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.

üß© –°–æ—Å—Ç–∞–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã  
- –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π, –∫–ª–∞—Å—Å–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ —Ñ–∞–π–ª–µ.  
- –î–ª—è –∫–∞–∂–¥–æ–≥–æ ‚Äî –∏–º—è –∏ 2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ –µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–∏ –∏ –∫–ª—é—á–µ–≤–æ–π –ª–æ–≥–∏–∫–µ.

üì• –í—Ö–æ–¥—ã –∏ üì§ –í—ã—Ö–æ–¥—ã  
- –ü–æ–¥—Ä–æ–±–Ω–æ —É–∫–∞–∂–∏, –æ—Ç–∫—É–¥–∞ –∏–º–µ–Ω–Ω–æ –ø–æ—Å—Ç—É–ø–∞—é—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏, HTTP‚Äë–∑–∞–ø—Ä–æ—Å, JSON‚Äë—Ñ–∞–π–ª).  
- –ö—É–¥–∞ –∏ –≤ –∫–∞–∫–æ–º –≤–∏–¥–µ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å, –∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª, –æ—Ç–ø—Ä–∞–≤–∫–∞ –ø–æ API).

üìù –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞  
- –í –∫–æ–Ω—Ü–µ –¥–∞–π 2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Ä–µ–∑—é–º–∏—Ä—É—é—â–∏–µ –ª–æ–≥–∏–∫—É —Ñ–∞–π–ª–∞.  
  –ù–∞–ø—Ä–∏–º–µ—Ä: "–ò—Ç–æ–≥–æ: —ç—Ç–æ—Ç —Ñ–∞–π–ª –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."

–ù–∞—á–∏–Ω–∞–π –æ—Ç—á—ë—Ç —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞  
`Audit Report: {filename}`  

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —è–≤–Ω–æ —Å–ª–µ–¥—É–µ—Ç –∏–∑ –∫–æ–¥–∞, –±–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤ –æ –≤–Ω–µ—à–Ω–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–∏.

–ö–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:

{code_content}
"""

MAX_RESPONSE_TOKENS = 2048  # —Ä–∞–Ω—å—à–µ –±—ã–ª–æ 1000


class GPTCache:
    """–ö—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã GPT‚Äë–∞–Ω–∞–ª–∏–∑–æ–≤"""

    def __init__(self, cache_dir: str = "./cache") -> None:
        self.dir = Path(cache_dir)
        self.dir.mkdir(parents=True, exist_ok=True)

    def get_cache_key(self, request: GPTAnalysisRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        content = f"{request.file_path}_{request.language}_"
        content += "_".join([chunk.content for chunk in request.chunks])
        return hashlib.md5(content.encode()).hexdigest()

    def clear_expired_cache(self, days: int = 7) -> None:
        """–û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ –∫—ç—à–∞"""
        cutoff = datetime.now() - timedelta(days=days)
        for cache_file in self.dir.glob("*.json"):
            try:
                data = json.loads(cache_file.read_text(encoding="utf-8"))
                cached_at = datetime.fromisoformat(data.get("cached_at", ""))
                if cached_at < cutoff:
                    cache_file.unlink()
                    logger.debug("–£–¥–∞–ª–µ–Ω —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à: %s", cache_file.name)
            except Exception as exc:
                logger.warning("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫—ç—à–∞ %s: %s", cache_file.name, exc)

    def get_cached_result(self, key: str) -> Optional[GPTAnalysisResult]:
        file = self.dir / f"{key}.json"
        if not file.exists():
            return None
        try:
            data = json.loads(file.read_text(encoding="utf-8"))
            return GPTAnalysisResult(
                summary=data.get("summary", ""),
                key_components=data.get("key_components", []),
                analysis_per_chunk=data.get("analysis_per_chunk", {}),
                error=data.get("error"),
                full_text=data.get("full_text", ""),
            )
        except Exception as exc:
            logger.warning("–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞ %s: %s", key, exc)
            return None

    def cache_result(self, key: str, res: GPTAnalysisResult) -> None:
        file = self.dir / f"{key}.json"
        data = {
            "summary": res.summary,
            "key_components": res.key_components,
            "analysis_per_chunk": res.analysis_per_chunk,
            "error": res.error,
            "full_text": res.full_text,
            "cached_at": datetime.now().isoformat(),
        }
        try:
            file.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        except Exception as exc:  # pragma: no cover
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –∫—ç—à %s: %s", key, exc)


class OpenAIManager:
    """–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å OpenAI"""

    def __init__(self) -> None:
        cfg = get_config()
        if not cfg.openai.api_key:
            raise ValueError("OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")
        self.client = OpenAI(api_key=cfg.openai.api_key)
        self.model = cfg.openai.model
        self.temperature = cfg.openai.temperature
        try:
            self.encoder = tiktoken.encoding_for_model(self.model)
        except Exception:
            self.encoder = tiktoken.get_encoding("cl100k_base")

        self.cache = GPTCache()

    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        return len(self.encoder.encode(text))

    def truncate_to_tokens(self, text: str, max_tokens: int) -> str:
        """–û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        tokens = self.encoder.encode(text)
        if len(tokens) <= max_tokens:
            return text
        truncated_tokens = tokens[:max_tokens]
        return self.encoder.decode(truncated_tokens)

    async def analyze_code(self, request: GPTAnalysisRequest) -> GPTAnalysisResult:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞ —á–µ—Ä–µ–∑ GPT"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cache_key = self.cache.get_cache_key(request)
            cached = self.cache.get_cached_result(cache_key)
            if cached:
                logger.debug("–ù–∞–π–¥–µ–Ω –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è %s", request.file_path)
                return cached

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–¥
            combined_code = self._combine_chunks(request.chunks)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = self._build_analysis_prompt(request, combined_code)
            
            # –í—ã–∑—ã–≤–∞–µ–º API
            response = await self._call_openai_api(prompt)
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            result = self._parse_gpt_response(response, request.chunks)
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.cache.cache_result(cache_key, result)
            
            return result

        except Exception as exc:
            error_msg = f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {request.file_path}: {str(exc)}"
            logger.error(error_msg)
            return GPTAnalysisResult(
                summary="–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
                key_components=[],
                analysis_per_chunk={},
                error=error_msg
            )

    def _combine_chunks(self, chunks: List[CodeChunk]) -> str:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∫–æ–¥–∞ –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç"""
        if not chunks:
            return ""
        
        # –ë–µ—Ä—ë–º –¥–æ 3-4 –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        important_chunks = chunks[:4]
        combined = []
        
        for chunk in important_chunks:
            combined.append(f"// --- {chunk.name} (—Å—Ç—Ä–æ–∫–∏ {chunk.start_line}-{chunk.end_line}) ---")
            combined.append(chunk.content)
            combined.append("")
        
        return "\n".join(combined)

    def _build_analysis_prompt(self, request: GPTAnalysisRequest, code: str) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        filename = Path(request.file_path).name
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        total_lines = code.count('\n') + 1 if code else 0
        functions_count = len([chunk for chunk in request.chunks if chunk.chunk_type == 'function'])
        classes_count = len([chunk for chunk in request.chunks if chunk.chunk_type == 'class'])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–¥–∞
        max_code_tokens = 1500  # –û—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞
        if self.count_tokens(code) > max_code_tokens:
            code = self.truncate_to_tokens(code, max_code_tokens)
            code += "\n\n... [–∫–æ–¥ –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤] ..."

        return MINIMAL_ANALYSIS_PROMPT.format(
            filename=filename,
            total_lines=total_lines,
            functions_count=functions_count,
            classes_count=classes_count,
            code_content=code
        )

    async def _call_openai_api(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∫–æ–¥–∞. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–π –∫—Ä–∞—Ç–∫–∏–µ –∏ —Ç–æ—á–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=self.temperature,
            max_tokens=MAX_RESPONSE_TOKENS,
        )
        return response.choices[0].message.content

    def _parse_gpt_response(self, text: str, chunks: List[CodeChunk]) -> GPTAnalysisResult:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ–º *–≤–µ—Å—å* text –≤ full_text, –∞ summary/keys ‚Äî –¥–ª—è –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏.
        """
        summary = ""
        key_components: List[str] = []

        for line in text.splitlines():
            if line.startswith("üîç") or line.startswith("–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:"):
                summary = line.lstrip("üîç ").replace("–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:", "").strip()
            if line.startswith("- ") and "–§—É–Ω–∫—Ü–∏—è" in line:
                key_components.append(line.lstrip("- ").strip())

        if not summary:
            summary = text[:200] + "..." if len(text) > 200 else text

        return GPTAnalysisResult(
            summary=summary,
            key_components=key_components,
            analysis_per_chunk={chunk.name: summary for chunk in chunks[:3]},
            full_text=text,
            error=None,
        )

    def get_token_usage_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (–∑–∞–≥–ª—É—à–∫–∞)"""
        return {
            "total_requests": 0,
            "total_tokens": 0,
            "average_tokens_per_request": 0
        }

    def clear_cache(self) -> int:
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ OpenAI"""
        cache_files = list(self.cache.dir.glob("*.json"))
        count = len(cache_files)
        for cache_file in cache_files:
            cache_file.unlink()
        return count
