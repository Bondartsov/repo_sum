#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π MD –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ OpenAI GPT.
–¢–µ–ø–µ—Ä—å –≤–∫–ª—é—á–∞–µ—Ç RAG —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–æ–¥—É.
"""

import asyncio
import logging
import sys
from pathlib import Path
from typing import List, Tuple

import click
from rich.console import Console
from rich.progress import Progress, TaskID, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeElapsedColumn
from rich.logging import RichHandler
from rich.table import Table

from config import get_config, reload_config
from file_scanner import FileScanner
from parsers.base_parser import ParserRegistry
from code_chunker import CodeChunker
from openai_integration import OpenAIManager
from doc_generator import DocumentationGenerator
from utils import (
    setup_logging, FileInfo, ParsedFile, GPTAnalysisRequest, GPTAnalysisResult,
    ensure_directory_exists, create_error_parsed_file, create_error_gpt_result,
    compute_file_hash, read_index, write_index
)

# RAG —Å–∏—Å—Ç–µ–º–∞
from rag.indexer_service import IndexerService
from rag.search_service import SearchService
from rag.exceptions import VectorStoreException, VectorStoreConnectionError


class RepositoryAnalyzer:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤"""
    
    def __init__(self):
        self.config = get_config()
        self.console = Console()
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.file_scanner = FileScanner()
        self.parser_registry = ParserRegistry()
        self.code_chunker = CodeChunker()
        self.openai_manager = OpenAIManager()
        self.doc_generator = DocumentationGenerator()
        
    async def analyze_repository(self, repo_path: str, output_path: str, show_progress: bool = True, incremental: bool = True) -> dict:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è"""
        self.logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è: {repo_path}")
        self.logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        ensure_directory_exists(output_path)
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
        self.console.print("[bold blue]–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...[/bold blue]")
        all_files = list(self.file_scanner.scan_repository(repo_path))

        # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º: –æ—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        files_to_analyze = all_files
        index_path = str(Path(repo_path) / ".repo_sum" / "index.json")
        if incremental:
            try:
                index = read_index(index_path)
                changed: List[FileInfo] = []
                for fi in all_files:
                    try:
                        h = compute_file_hash(fi.path)
                    except Exception:
                        # –µ—Å–ª–∏ –Ω–µ —É–¥–∞—ë—Ç—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª ‚Äî —Å—á–∏—Ç–∞–µ–º –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–º
                        changed.append(fi)
                        continue
                    prev = index.get(fi.path, {})
                    if prev.get("hash") != h:
                        changed.append(fi)
                if changed:
                    files_to_analyze = changed
                else:
                    self.console.print("[green]–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π ‚Äî –æ—Ç—á—ë—Ç—ã –∞–∫—Ç—É–∞–ª—å–Ω—ã[/green]")
                    return {
                        'total_files': 0,
                        'successful': 0,
                        'failed': 0,
                        'output_directory': str(Path(output_path) / f"SUMMARY_REPORT_{Path(repo_path).name}"),
                        'index_file': str(Path(output_path) / f"SUMMARY_REPORT_{Path(repo_path).name}" / "README.md"),
                        'success': True
                    }
            except Exception as e:
                self.logger.warning(f"–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∫–ª—é—á–µ–Ω –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
                files_to_analyze = all_files
        
        if not files_to_analyze:
            self.console.print("[bold red]–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞![/bold red]")
            return {'success': False, 'error': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'}
        
        self.console.print(f"[green]–ù–∞–π–¥–µ–Ω–æ {len(files_to_analyze)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞[/green]")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —è–∑—ã–∫–∞–º
        self._show_language_statistics(files_to_analyze)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        if show_progress:
            analyzed_files = await self._analyze_files_with_progress(files_to_analyze)
        else:
            analyzed_files = await self._analyze_files_simple(files_to_analyze)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
        self.console.print("[bold blue]–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏...[/bold blue]")
        result = self.doc_generator.generate_complete_documentation(
            analyzed_files, output_path, repo_path
        )

        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        try:
            index = read_index(index_path)
            now = __import__('datetime').datetime.utcnow().isoformat()
            for parsed_file, _ in analyzed_files:
                try:
                    h = compute_file_hash(parsed_file.file_info.path)
                    index[parsed_file.file_info.path] = {"hash": h, "analyzed_at": now}
                except Exception:
                    continue
            write_index(index_path, index)
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∏–∑–º–µ–Ω–µ–Ω–∏–π: {e}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._show_final_statistics(result)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–∫–µ–Ω–æ–≤
        token_stats = self.openai_manager.get_token_usage_stats()
        self._show_token_statistics(token_stats)
        
        return result
    
    async def _analyze_files_with_progress(self, files: List[FileInfo]) -> List[Tuple[ParsedFile, GPTAnalysisResult]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞, –∏—Å–ø–æ–ª—å–∑—É—è –±–∞—Ç—á–µ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É"""
        analyzed_files = []
        batch_size = self._get_optimal_batch_size(len(files))
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=self.console
        ) as progress:
            
            task = progress.add_task("–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤...", total=len(files))
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –±–∞—Ç—á–∞–º–∏
            for i in range(0, len(files), batch_size):
                batch = files[i:i + batch_size]
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
                batch_names = [Path(f.path).name for f in batch[:2]]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 2 —Ñ–∞–π–ª–∞
                if len(batch) > 2:
                    batch_names.append(f"–∏ –µ—â–µ {len(batch) - 2}")
                progress.update(task, description=f"–ë–∞—Ç—á: {', '.join(batch_names)}")
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞—Ç—á –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
                batch_results = await self._analyze_files_batch(batch)
                analyzed_files.extend(batch_results)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                progress.advance(task, len(batch))
        
        return analyzed_files
    
    def _get_optimal_batch_size(self, total_files: int) -> int:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤"""
        if total_files <= 10:
            return 2  # –ú–∞–ª–µ–Ω—å–∫–∏–µ –ø—Ä–æ–µ–∫—Ç—ã - –Ω–µ–±–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏
        elif total_files <= 50:
            return 3  # –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç—ã
        elif total_files <= 200:
            return 5  # –ë–æ–ª—å—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã
        else:
            return 8  # –û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã
    
    async def _analyze_files_batch(self, batch: List[FileInfo]) -> List[Tuple[ParsedFile, GPTAnalysisResult]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞—Ç—á —Ñ–∞–π–ª–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        tasks = [self._analyze_single_file_safe(file_info) for file_info in batch]
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        analyzed_files = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {batch[i].path}: {result}")
                error_parsed = create_error_parsed_file(batch[i], result)
                error_gpt = create_error_gpt_result(result) 
                analyzed_files.append((error_parsed, error_gpt))
            else:
                analyzed_files.append(result)
        
        return analyzed_files
    
    async def _analyze_single_file_safe(self, file_info: FileInfo) -> Tuple[ParsedFile, GPTAnalysisResult]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
        try:
            return await self._analyze_single_file(file_info)
        except Exception as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É, –Ω–æ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {file_info.path}: {e}")
            error_parsed = create_error_parsed_file(file_info, e)
            error_gpt = create_error_gpt_result(e)
            return (error_parsed, error_gpt)
    
    async def _analyze_files_simple(self, files: List[FileInfo]) -> List[Tuple[ParsedFile, GPTAnalysisResult]]:
        """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞, —Ç–æ–∂–µ —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        analyzed_files = []
        batch_size = self._get_optimal_batch_size(len(files))
        
        for i in range(0, len(files), batch_size):
            batch = files[i:i + batch_size]
            batch_start = i + 1
            batch_end = min(i + batch_size, len(files))
            
            self.console.print(f"[dim]–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ {batch_start}-{batch_end}/{len(files)}[/dim]")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –±–∞—Ç—á–µ–≤—É—é –ª–æ–≥–∏–∫—É
            batch_results = await self._analyze_files_batch(batch)
            analyzed_files.extend(batch_results)
        
        return analyzed_files
    
    async def _analyze_single_file(self, file_info: FileInfo) -> Tuple[ParsedFile, GPTAnalysisResult]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª"""
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä—Å–µ—Ä
        parser = self.parser_registry.get_parser(file_info.path)
        if not parser:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Ñ–∞–π–ª–∞ {file_info.path}")
        
        # –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª
        parsed_file = parser.safe_parse(file_info)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.code_chunker.chunk_parsed_file(parsed_file)
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ GPT
        gpt_request = GPTAnalysisRequest(
            file_path=file_info.path,
            language=file_info.language,
            chunks=chunks
        )
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ GPT
        gpt_result = await self.openai_manager.analyze_code(gpt_request)
        
        return parsed_file, gpt_result
    
    def _show_language_statistics(self, files: List[FileInfo]) -> None:
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"""
        languages = {}
        total_size = 0
        
        for file_info in files:
            lang = file_info.language
            languages[lang] = languages.get(lang, {'count': 0, 'size': 0})
            languages[lang]['count'] += 1
            languages[lang]['size'] += file_info.size
            total_size += file_info.size
        
        table = Table(title="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º")
        table.add_column("–Ø–∑—ã–∫", style="cyan", no_wrap=True)
        table.add_column("–§–∞–π–ª–æ–≤", justify="right", style="green")
        table.add_column("–†–∞–∑–º–µ—Ä", justify="right", style="yellow")
        table.add_column("%", justify="right", style="blue")
        
        for lang in sorted(languages.keys()):
            stats = languages[lang]
            size_str = self._format_size(stats['size'])
            percentage = (stats['size'] / total_size * 100) if total_size > 0 else 0
            
            table.add_row(
                lang.title(),
                str(stats['count']),
                size_str,
                f"{percentage:.1f}%"
            )
        
        self.console.print(table)
    
    def _show_final_statistics(self, result: dict) -> None:
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        table = Table(title="–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞")
        table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
        table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", justify="right", style="green")
        
        table.add_row("–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤", str(result.get('total_files', 0)))
        table.add_row("–£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ", str(result.get('successful', 0)))
        
        if result.get('failed', 0) > 0:
            table.add_row("–° –æ—à–∏–±–∫–∞–º–∏", str(result['failed']), style="red")
        
        table.add_row("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –≤—ã–≤–æ–¥–∞", str(result.get('output_directory', '')))
        
        self.console.print(table)
    
    def _show_token_statistics(self, token_stats: dict) -> None:
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤"""
        table = Table(title="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ OpenAI")
        table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
        table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", justify="right", style="yellow")
        
        table.add_row("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Å–µ–≥–æ–¥–Ω—è", str(token_stats.get('used_today', 0)))
        
        self.console.print(table)
    
    def _format_size(self, size_bytes: int) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        else:
            return f"{size_bytes / (1024 * 1024):.1f} MB"


# CLI –∫–æ–º–∞–Ω–¥—ã
@click.group()
@click.option('--config', '-c', default='settings.json', help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
@click.option('--verbose', '-v', is_flag=True, help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥')
@click.option('--quiet', '-q', is_flag=True, help='–¢–∏—Ö–∏–π —Ä–µ–∂–∏–º')
@click.pass_context
def cli(ctx, config, verbose, quiet):
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π MD –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ OpenAI GPT."""
    ctx.ensure_object(dict)
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    if quiet:
        log_level = "ERROR"
    elif verbose:
        log_level = "DEBUG"
    else:
        log_level = "INFO"
    
    setup_logging(log_level)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    try:
        if config != 'settings.json':
            reload_config(config)
        else:
            get_config()  # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    except Exception as e:
        console = Console()
        console.print(f"[bold red]–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}[/bold red]")
        sys.exit(1)


@cli.command()
@click.argument('repo_path', type=click.Path(exists=True))
@click.option('--output', '-o', default='./docs', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏')
@click.option('--no-progress', is_flag=True, help='–û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä')
@click.option('--incremental/--no-incremental', default=True, help='–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤')
def analyze(repo_path, output, no_progress, incremental):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏ —Å–æ–∑–¥–∞–µ—Ç MD –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é."""
    console = Console()
    
    try:
        # Fail-fast: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
        analyzer = RepositoryAnalyzer()
    except ValueError as e:
        # –ë—ã—Å—Ç—Ä—ã–π –≤—ã—Ö–æ–¥ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ API –∫–ª—é—á–∞)
        console.print(f"[bold red]–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}[/bold red]")
        sys.exit(1)
    except Exception as e:
        # –õ—é–±–∞—è –¥—Ä—É–≥–∞—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        logger = logging.getLogger(__name__)
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}", exc_info=True)
        console.print(f"[bold red]–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}[/bold red]")
        sys.exit(1)
    
    try:
        result = asyncio.run(analyzer.analyze_repository(
            repo_path, output, show_progress=not no_progress, incremental=incremental
        ))
        
        if result.get('success', True):
            console = Console()
            console.print(f"[bold green]–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ![/bold green]")
            saved_dir = result.get('output_directory', output)
            console.print(f"–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: [cyan]{saved_dir}[/cyan]")
            if result.get('index_file'):
                console.print(f"–ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª: [cyan]{result['index_file']}[/cyan]")
        else:
            console = Console()
            console.print(f"[bold red]–û—à–∏–±–∫–∞: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}[/bold red]")
            sys.exit(1)
            
    except KeyboardInterrupt:
        console = Console()
        console.print("[yellow]–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º[/yellow]")
        sys.exit(1)
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        console = Console()
        console.print(f"[bold red]–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}[/bold red]")
        sys.exit(1)


@cli.command()
@click.argument('repo_path', type=click.Path(exists=True))
def stats(repo_path):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞."""
    console = Console()
    console.print(f"[bold blue]–°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è: {repo_path}[/bold blue]")
    
    try:
        scanner = FileScanner()
        stats = scanner.get_repository_stats(repo_path)
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        table = Table(title="–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
        table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", justify="right", style="green")
        
        table.add_row("–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤", str(stats['total_files']))
        table.add_row("–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä", RepositoryAnalyzer()._format_size(stats['total_size']))
        
        console.print(table)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º
        if stats['languages']:
            lang_table = Table(title="–ü–æ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è")
            lang_table.add_column("–Ø–∑—ã–∫", style="cyan")
            lang_table.add_column("–§–∞–π–ª–æ–≤", justify="right", style="green")
            
            for lang, count in sorted(stats['languages'].items(), key=lambda x: x[1], reverse=True):
                lang_table.add_row(lang.title(), str(count))
            
            console.print(lang_table)
        
        # –°–∞–º—ã–µ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã
        if stats['largest_files']:
            large_table = Table(title="–°–∞–º—ã–µ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã")
            large_table.add_column("–§–∞–π–ª", style="cyan")
            large_table.add_column("–†–∞–∑–º–µ—Ä", justify="right", style="yellow")
            large_table.add_column("–Ø–∑—ã–∫", style="green")
            
            for file_info in stats['largest_files'][:10]:
                large_table.add_row(
                    str(Path(file_info['path']).name),
                    RepositoryAnalyzer()._format_size(file_info['size']),
                    file_info['language'].title()
                )
            
            console.print(large_table)
            
    except Exception as e:
        console.print(f"[bold red]–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}[/bold red]")
        sys.exit(1)


@cli.command()
def clear_cache():
    """–û—á–∏—â–∞–µ—Ç –∫—ç—à OpenAI –∑–∞–ø—Ä–æ—Å–æ–≤."""
    try:
        manager = OpenAIManager()
        cleared = manager.clear_cache()
        
        console = Console()
        console.print(f"[green]–û—á–∏—â–µ–Ω–æ {cleared} –∑–∞–ø–∏—Å–µ–π –∫—ç—à–∞[/green]")
        
    except Exception as e:
        console = Console()
        console.print(f"[bold red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∫—ç—à–∞: {e}[/bold red]")


@cli.command()
def token_stats():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ OpenAI."""
    try:
        manager = OpenAIManager()
        stats = manager.get_token_usage_stats()
        
        console = Console()
        table = Table(title="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ OpenAI")
        table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
        table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", justify="right", style="green")
        
        table.add_row("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Å–µ–≥–æ–¥–Ω—è", str(stats['used_today']))
        
        console.print(table)
        
    except Exception as e:
        console = Console()
        console.print(f"[bold red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}[/bold red]")


# RAG –∫–æ–º–∞–Ω–¥—ã
@cli.group()
def rag():
    """–ö–æ–º–∞–Ω–¥—ã RAG —Å–∏—Å—Ç–µ–º—ã –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–æ–¥—É"""
    pass


@rag.command()
@click.argument('repo_path', type=click.Path(exists=True))
@click.option('--batch-size', default=512, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏')
@click.option('--recreate', is_flag=True, help='–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é')
@click.option('--no-progress', is_flag=True, help='–û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä')
def index(repo_path, batch_size, recreate, no_progress):
    """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î"""
    console = Console()
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π RAG –Ω–∞—Å—Ç—Ä–æ–µ–∫
        config = get_config()
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º RAG –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        try:
            config.validate(require_api_key=False)
        except ValueError as e:
            console.print(f"[bold red]–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ RAG: {e}[/bold red]")
            sys.exit(1)
        
        console.print(f"[bold blue]üîÑ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è: {repo_path}[/bold blue]")
        console.print(f"[dim]Qdrant: {config.rag.vector_store.host}:{config.rag.vector_store.port}[/dim]")
        console.print(f"[dim]–ö–æ–ª–ª–µ–∫—Ü–∏—è: {config.rag.vector_store.collection_name}[/dim]")
        console.print(f"[dim]–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}[/dim]")
        
        if recreate:
            console.print("[yellow]‚ö†Ô∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è –±—É–¥–µ—Ç –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞[/yellow]")
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        indexer = IndexerService(config)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
        result = asyncio.run(indexer.index_repository(
            repo_path=repo_path,
            batch_size=batch_size,
            recreate=recreate,
            show_progress=not no_progress
        ))
        
        if result.get('success', False):
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            table = Table(title="–†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
            table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", justify="right", style="green")
            
            table.add_row("–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤", str(result.get('total_files', 0)))
            table.add_row("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤", str(result.get('processed_files', 0)))
            table.add_row("–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤", str(result.get('total_chunks', 0)))
            table.add_row("–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ —á–∞–Ω–∫–æ–≤", str(result.get('indexed_chunks', 0)))
            table.add_row("–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", f"{result.get('total_time', 0):.1f}s")
            table.add_row("–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏", f"{result.get('processing_rate', 0):.1f} —Ñ–∞–π–ª–æ–≤/—Å")
            table.add_row("–°–∫–æ—Ä–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏", f"{result.get('indexing_rate', 0):.1f} —á–∞–Ω–∫–æ–≤/—Å")
            
            if result.get('failed_files', 0) > 0:
                table.add_row("–° –æ—à–∏–±–∫–∞–º–∏", str(result['failed_files']), style="red")
            
            console.print(table)
            console.print(f"[bold green]‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ![/bold green]")
        else:
            console.print(f"[bold red]‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}[/bold red]")
            sys.exit(1)
            
    except KeyboardInterrupt:
        console.print("[yellow]‚èπÔ∏è –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º[/yellow]")
        sys.exit(1)
    except VectorStoreConnectionError as e:
        console.print(f"[bold red]‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant: {e}[/bold red]")
        console.print("[dim]–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ Qdrant –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω[/dim]")
        sys.exit(1)
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}", exc_info=True)
        console.print(f"[bold red]‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}[/bold red]")
        sys.exit(1)


@rag.command()
@click.argument('query')
@click.option('--top-k', default=10, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
@click.option('--lang', help='–§–∏–ª—å—Ç—Ä –ø–æ —è–∑—ã–∫—É –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è')
@click.option('--chunk-type', help='–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É —á–∞–Ω–∫–∞ (class, function, etc.)')
@click.option('--min-score', type=float, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0.0-1.0)')
@click.option('--file-path', help='–§–∏–ª—å—Ç—Ä –ø–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É')
@click.option('--no-content', is_flag=True, help='–ù–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ')
@click.option('--max-lines', default=10, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞')
def search(query, top_k, lang, chunk_type, min_score, file_path, no_content, max_lines):
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–¥—É"""
    console = Console()
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = get_config()
        
        console.print(f"[bold blue]üîç –ü–æ–∏—Å–∫: '{query}'[/bold blue]")
        console.print(f"[dim]–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {top_k}, –§–∏–ª—å—Ç—Ä—ã: —è–∑—ã–∫={lang or '–≤—Å–µ'}, —Ç–∏–ø={chunk_type or '–≤—Å–µ'}[/dim]")
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å –ø–æ–∏—Å–∫–∞
        searcher = SearchService(config)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        results = asyncio.run(searcher.search(
            query=query,
            top_k=top_k,
            language_filter=lang,
            chunk_type_filter=chunk_type,
            min_score=min_score,
            file_path_filter=file_path
        ))
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏ –≤—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        searcher.format_search_results(
            results=results,
            show_content=not no_content,
            max_content_lines=max_lines
        )
        
        if results:
            console.print()
            console.print(f"[dim]–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä: {sum(r.score for r in results) / len(results):.3f}[/dim]")
        
    except KeyboardInterrupt:
        console.print("[yellow]‚èπÔ∏è –ü–æ–∏—Å–∫ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º[/yellow]")
        sys.exit(1)
    except VectorStoreConnectionError as e:
        console.print(f"[bold red]‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant: {e}[/bold red]")
        console.print("[dim]–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ Qdrant –∑–∞–ø—É—â–µ–Ω –∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞[/dim]")
        sys.exit(1)
    except VectorStoreException as e:
        console.print(f"[bold red]‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}[/bold red]")
        sys.exit(1)
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}", exc_info=True)
        console.print(f"[bold red]‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}[/bold red]")
        sys.exit(1)


@rag.command()
@click.option('--detailed', is_flag=True, help='–ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞')
def status(detailed):
    """–°—Ç–∞—Ç—É—Å RAG —Å–∏—Å—Ç–µ–º—ã –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î"""
    console = Console()
    
    try:
        config = get_config()
        
        console.print("[bold blue]üìä –°—Ç–∞—Ç—É—Å RAG —Å–∏—Å—Ç–µ–º—ã[/bold blue]")
        
        # Health check –∏–Ω–¥–µ–∫—Å–µ—Ä–∞
        indexer = IndexerService(config)
        health = asyncio.run(indexer.health_check())
        
        # –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å
        status_color = "green" if health['status'] == 'healthy' else "yellow" if health['status'] == 'degraded' else "red"
        console.print(f"[bold]–û–±—â–∏–π —Å—Ç–∞—Ç—É—Å: [{status_color}]{health['status'].upper()}[/{status_color}][/bold]")
        console.print()
        
        # –°—Ç–∞—Ç—É—Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        components_table = Table(title="–°—Ç–∞—Ç—É—Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤")
        components_table.add_column("–ö–æ–º–ø–æ–Ω–µ–Ω—Ç", style="cyan")
        components_table.add_column("–°—Ç–∞—Ç—É—Å", style="bold")
        components_table.add_column("–î–µ—Ç–∞–ª–∏", style="dim")
        
        # Vector Store
        vs_health = health['components'].get('vector_store', {})
        vs_status = vs_health.get('status', 'unknown')
        vs_status_color = "green" if vs_status == 'connected' else "red"
        
        vs_details = ""
        if vs_health.get('collection_info'):
            coll_info = vs_health['collection_info']
            vs_details = f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {coll_info.get('points_count', 0)}, –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {coll_info.get('indexed_vectors_count', 0)}"
        
        components_table.add_row(
            "Qdrant Vector Store",
            f"[{vs_status_color}]{vs_status}[/{vs_status_color}]",
            vs_details
        )
        
        # Embedder
        embedder_health = health['components'].get('embedder', {})
        embedder_status = embedder_health.get('status', 'unknown')
        embedder_status_color = "green" if embedder_status == 'healthy' else "yellow"
        embedder_details = f"–ú–æ–¥–µ–ª—å: {embedder_health.get('model', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}, –ü—Ä–æ–≤–∞–π–¥–µ—Ä: {embedder_health.get('provider', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
        
        components_table.add_row(
            "Embedder",
            f"[{embedder_status_color}]{embedder_status}[/{embedder_status_color}]",
            embedder_details
        )
        
        console.print(components_table)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        console.print()
        config_table = Table(title="–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
        config_table.add_column("–ü–∞—Ä–∞–º–µ—Ç—Ä", style="cyan")
        config_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="green")
        
        config_table.add_row("–•–æ—Å—Ç Qdrant", f"{config.rag.vector_store.host}:{config.rag.vector_store.port}")
        config_table.add_row("–ö–æ–ª–ª–µ–∫—Ü–∏—è", config.rag.vector_store.collection_name)
        config_table.add_row("–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤", str(config.rag.vector_store.vector_size))
        config_table.add_row("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", config.rag.embeddings.model_name)
        config_table.add_row("–ü—Ä–æ–≤–∞–π–¥–µ—Ä", config.rag.embeddings.provider)
        config_table.add_row("–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ", f"{config.rag.vector_store.quantization_type}" if config.rag.vector_store.enable_quantization else "–æ—Ç–∫–ª—é—á–µ–Ω–æ")
        
        console.print(config_table)
        
        # –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if detailed:
            console.print()
            stats = asyncio.run(indexer.get_indexing_stats())
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            if stats.get('indexer'):
                indexer_stats = stats['indexer']
                index_table = Table(title="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
                index_table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
                index_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="green")
                
                index_table.add_row("–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤", str(indexer_stats.get('total_files', 0)))
                index_table.add_row("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤", str(indexer_stats.get('processed_files', 0)))
                index_table.add_row("–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤", str(indexer_stats.get('total_chunks', 0)))
                index_table.add_row("–û—à–∏–±–æ–∫", str(indexer_stats.get('failed_files', 0)))
                
                console.print(index_table)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–¥–µ—Ä–∞
            if stats.get('embedder'):
                embedder_stats = stats['embedder']
                embed_table = Table(title="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–¥–µ—Ä–∞")
                embed_table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
                embed_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="green")
                
                embed_table.add_row("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤", str(embedder_stats.get('total_texts', 0)))
                embed_table.add_row("–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –±–∞—Ç—á–∞", f"{embedder_stats.get('avg_batch_time', 0):.3f}s")
                embed_table.add_row("–¢–µ–∫—Å—Ç–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É", f"{embedder_stats.get('avg_texts_per_second', 0):.1f}")
                embed_table.add_row("–¢–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞", str(embedder_stats.get('current_batch_size', 0)))
                embed_table.add_row("OOM fallbacks", str(embedder_stats.get('oom_fallbacks', 0)))
                
                console.print(embed_table)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞
            searcher = SearchService(config)
            search_stats = searcher.get_search_stats()
            
            search_table = Table(title="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞")
            search_table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
            search_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="green")
            
            search_table.add_row("–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤", str(search_stats.get('total_queries', 0)))
            search_table.add_row("–ü–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫—ç—à", str(search_stats.get('cache_hits', 0)))
            search_table.add_row("–ü—Ä–æ–º–∞—Ö–æ–≤ –∫—ç—à–∞", str(search_stats.get('cache_misses', 0)))
            search_table.add_row("–†–∞–∑–º–µ—Ä –∫—ç—à–∞", str(search_stats.get('cache_size', 0)))
            search_table.add_row("–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞", f"{search_stats.get('avg_search_time', 0):.3f}s")
            
            console.print(search_table)
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Ä–≤–∏—Å—ã
        asyncio.run(indexer.close())
        
    except KeyboardInterrupt:
        console.print("[yellow]‚èπÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º[/yellow]")
        sys.exit(1)
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞: {e}", exc_info=True)
        console.print(f"[bold red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞: {e}[/bold red]")
        sys.exit(1)


if __name__ == '__main__':
    cli()
